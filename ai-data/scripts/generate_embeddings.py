import os
import re
import json
import hashlib
import pickle
from typing import Dict, Any, List, Optional
from pathlib import Path

import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm
from openai import OpenAI
from langchain_core.documents import Document

# =========================================
# .env ë¡œë“œ
# =========================================
load_dotenv()

# =========================================
# ìœ í‹¸
# =========================================
def _norm(s: Any) -> str:
    s = "" if s is None else str(s)
    s = s.strip()
    s = re.sub(r"\s+", " ", s)
    return s

def make_stable_id(brand: str, name: str) -> str:
    """ë¸Œëœë“œ+ì´ë¦„ ê¸°ë°˜ ì•ˆì •ì  ID"""
    base = f"{brand.strip()}::{name.strip()}".lower()
    hid = hashlib.sha1(base.encode("utf-8")).hexdigest()[:16]
    return f"perfume_{hid}"

class EmbeddingGenerator:
    def __init__(self):
        """OpenAI ì´ˆê¸°í™” & ì„¤ì •"""
        self.openai_api_key = os.getenv("OPENAI_API_KEY")

        if not self.openai_api_key:
            raise ValueError("âŒ OPENAI_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")

        print("âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ")

        # OpenAI
        try:
            self.openai = OpenAI(api_key=self.openai_api_key)
            print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            raise ValueError(f"âŒ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # ===== ì„¤ì • =====
        self.embedding_model = "text-embedding-3-small"
        self.embed_batch_size = 128

    # -------------------------------------
    # CSV â†’ Document
    # -------------------------------------
    def parse_score_string(self, score_str: str) -> Optional[str]:
        if pd.isna(score_str) or not str(score_str).strip() or str(score_str).lower() == "nan":
            return None
        try:
            s = str(score_str).strip()
            scores: Dict[str, float] = {}
            if "(" in s and ")" in s:
                pattern = r"(\w+)\s*\(\s*([\d.]+)\s*\)"
                for key, val in re.findall(pattern, s):
                    try:
                        scores[key.strip()] = float(val.strip())
                    except ValueError:
                        continue
            elif s.startswith("{") and s.endswith("}"):
                try:
                    d = json.loads(s)
                    for k, v in d.items():
                        if isinstance(v, str):
                            cv = v.replace("%", "").strip()
                            if cv:
                                scores[str(k)] = float(cv)
                        elif isinstance(v, (int, float)):
                            scores[str(k)] = float(v)
                except json.JSONDecodeError:
                    pass
            return max(scores, key=scores.get) if scores else None
        except Exception:
            return None

    def csv_to_documents(self, csv_path: str) -> List[Document]:
        # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬: scripts/ ì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ../data/ ì°¸ì¡°
        if not os.path.isabs(csv_path):
            csv_path = os.path.join(os.path.dirname(__file__), "..", "data", csv_path)
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

        print(f"ğŸ“– CSV ë¡œë”©: {csv_path}")
        df = pd.read_csv(csv_path)
        print(f"ğŸ“Š í–‰ {len(df)}ê°œ")

        docs: List[Document] = []
        for _, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ”„ Document ìƒì„±"):
            description = str(row.get("description", "")).strip()
            if not description or description.lower() == "nan":
                continue

            season_top = self.parse_score_string(str(row.get("season_score", "")))
            daynight_top = self.parse_score_string(str(row.get("day_night_score", "")))

            brand = _norm(row.get("brand", ""))
            name = _norm(row.get("name", ""))

            meta: Dict[str, Any] = {
                "id": make_stable_id(brand, name),
                "brand": brand,
                "name": name,
                "concentration": _norm(row.get("concentration", "")),
                "gender": _norm(row.get("gender", "")),
                "sizes": _norm(row.get("sizes", "")),
            }
            if season_top:
                meta["season_score"] = season_top
            if daynight_top:
                meta["day_night_score"] = daynight_top

            docs.append(Document(page_content=description, metadata=meta))

        print(f"âœ… Document {len(docs)}ê°œ ìƒì„± ì™„ë£Œ")
        return docs

    # -------------------------------------
    # ë°°ì¹˜ ì„ë² ë”©
    # -------------------------------------
    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        resp = self.openai.embeddings.create(model=self.embedding_model, input=texts)
        return [item.embedding for item in resp.data]

    def documents_to_vectors_batched(self, docs: List[Document]) -> List[Dict]:
        vectors: List[Dict] = []
        print(f"ğŸ”„ ì„ë² ë”©(ë°°ì¹˜) ìƒì„±: batch={self.embed_batch_size}")
        for i in tqdm(range(0, len(docs), self.embed_batch_size), desc="ğŸ§® ì„ë² ë”© ë°°ì¹˜"):
            batch_docs = docs[i : i + self.embed_batch_size]
            texts = [d.page_content for d in batch_docs]
            try:
                embs = self.embed_batch(texts)
                for d, emb in zip(batch_docs, embs):
                    meta = dict(d.metadata)
                    meta["text"] = d.page_content
                    vectors.append({"id": meta["id"], "values": emb, "metadata": meta})
            except Exception as e:
                print(f"âš ï¸ ì„ë² ë”© ë°°ì¹˜ ì‹¤íŒ¨ (i={i}): {e}")
                continue
        print(f"âœ… ë²¡í„° {len(vectors)}ê°œ ìƒì„± ì™„ë£Œ")
        return vectors

    # -------------------------------------
    # íŒŒì¼ ì €ì¥
    # -------------------------------------
    def save_vectors(self, vectors: List[Dict], output_path: str) -> None:
        """ë²¡í„°ë¥¼ pickle íŒŒì¼ë¡œ ì €ì¥"""
        print(f"ğŸ’¾ ë²¡í„° ì €ì¥ ì¤‘: {output_path}")
        with open(output_path, "wb") as f:
            pickle.dump(vectors, f)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {len(vectors)}ê°œ ë²¡í„°")

        # JSONìœ¼ë¡œë„ ì €ì¥ (ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆë„ë¡, ë²¡í„°ëŠ” ì œì™¸)
        json_path = output_path.replace(".pkl", "_metadata.json")
        metadata_only = [
            {"id": v["id"], "metadata": v["metadata"]} for v in vectors[:10]
        ]  # ìƒ˜í”Œë§Œ
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(metadata_only, f, ensure_ascii=False, indent=2)
        print(f"âœ… ë©”íƒ€ë°ì´í„° ìƒ˜í”Œ ì €ì¥: {json_path}")

    # -------------------------------------
    # ì‹¤í–‰
    # -------------------------------------
    def run(self, csv_path: str, output_path: str) -> None:
        print("ğŸš€ ì„ë² ë”© ìƒì„± ì‹œì‘!\n")

        # (1) CSVâ†’Documents
        docs = self.csv_to_documents(csv_path)
        if not docs:
            print("âŒ ë³€í™˜í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # (2) Documentsâ†’Vectors (ë°°ì¹˜ ì„ë² ë”©)
        vectors = self.documents_to_vectors_batched(docs)
        if not vectors:
            print("âŒ ìƒì„±í•  ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # (3) íŒŒì¼ë¡œ ì €ì¥
        self.save_vectors(vectors, output_path)

        print("\nğŸ‰ ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼: {output_path}")
        print(f"ğŸ“Š ë²¡í„° ìˆ˜: {len(vectors)}")


# =========================================
# ë©”ì¸
# =========================================
def main():
    csv_file = "perfume_final.csv"
    output_file = "perfume_embeddings.pkl"

    try:
        generator = EmbeddingGenerator()
        generator.run(csv_file, output_file)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()
